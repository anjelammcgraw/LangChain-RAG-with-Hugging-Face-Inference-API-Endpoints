{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjelammcgraw/LangChain-with-Hugging-Face-Inference-API-Endpoints/blob/main/13_LangChain_with_Hugging_Face_Inference_API_Endpoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n"
      ],
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Set-up Hugging Face Infrence Endpoints\n",
        "Links to each endpoint can be found in this notebook"
      ],
      "metadata": {
        "id": "ENUY6OSnDy7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries\n"
      ],
      "metadata": {
        "id": "-spIWt2J3Quk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ],
      "metadata": {
        "id": "EwGLnp31jXJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46763be9-3d89-4d8b-a73a-053ac187df8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ],
      "metadata": {
        "id": "FMJqq8SYt34V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28af9af4-924e-4167-9ebc-487d2d317929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Environment Variables\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SpZTBLwK3TIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ],
      "metadata": {
        "id": "NspG8I0XlFTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee542e96-45fa-4002-d56e-d3fd82ee615a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace Write Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "2be54238-1506-4e32-e437-ffd0557297a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing our Hugging Face Inference Endpoint\n"
      ],
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_api_gateway = \"https://shqstbj42kn40tcx.us-east-1.aws.endpoints.huggingface.cloud\""
      ],
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "-fVaR1onmtkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a630148d-3e6b-4128-d771-cd8158a3a392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \" I'm doing well, thanks for asking! *smiles* It's great to see you here! *nods* Is there anything new you'd like to talk about or ask? I'm all ears! *winks*\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating LangChain components powered by the endpoints\n"
      ],
      "metadata": {
        "id": "mXTBnBTy3b62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceEndpoint for LLM\n"
      ],
      "metadata": {
        "id": "fd5DaxGEFohF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vc7K1rFhSVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6e2e0d-156f-4ba4-b723-ecd37bfebe61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ],
      "metadata": {
        "id": "mMJrWnKISFqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "be22a7ba-1c33-4e42-c3ae-c74f159f666f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nI'm feeling a bit under the weather at the moment.\\n\\n*sneezes* Excuse me! *sniffles*\\n\\nI think I might have a cold or something.\\n\\n*hacks into a nearby tissue*\\n\\nI'm just going to go and rest for a bit. *stumbles off*\\n\\nI hope you're feeling better soon! *mumbles*\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n"
      ],
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_api_gateway = \"https://uokrykiccx0nzwnd.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ],
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ],
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a29GQIaIBeFV",
        "outputId": "62c991e2-ee02-4fdf-f6d8-7d81aab17f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "api_key=SecretStr('**********') model_name='sentence-transformers/all-MiniLM-L6-v2' api_url='https://uokrykiccx0nzwnd.us-east-1.aws.endpoints.huggingface.cloud'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ],
      "metadata": {
        "id": "HvF_eMZZKnlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Retrieving data from Arxiv\n"
      ],
      "metadata": {
        "id": "P9pLgHfR3uY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ],
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_chunks)"
      ],
      "metadata": {
        "id": "d9BO1Y1Xur0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b6cb1d-7222-484b-b1eb-5051e63c489c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ],
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ],
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "#### ✅ Answer #2\n",
        "Limiting batch sizes when sending data to Hugging Face endpoints is crucial due to the constraints imposed by the models. Most models can handle sequences of up to 512 or 1024 tokens, and exceeding these limits can lead to memory issues or performance degradation.\n"
      ],
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ],
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
      ],
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ],
      "metadata": {
        "id": "0DwHoaIDQQ9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a440a6-2fc6-4c2f-9bb4-940d340b216b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Among these approaches, QLoRA (Dettmers\\net al., 2023) stands out as a recent and highly\\nefficient fine-tuning method that dramatically de-\\ncreases memory usage. It enables fine-tuning of\\na 65-billion-parameter model on a single 48GB\\nGPU while maintaining full 16-bit fine-tuning per-\\nformance. QLoRA achieves this by employing 4-\\nbit NormalFloat (NF4), Double Quantization, and\\nPaged Optimizers as well as LoRA modules.\\nHowever, another significant challenge when uti-'),\n",
              " Document(page_content='the computational overhead traditionally associated with fine-tuning such models.\\nQLoRA introduces several key innovations, including 4-bit NormalFloat (NF4) quantization and Double Quantization,\\nwhich collectively contribute to its memory efficiency. These techniques enable the fine-tuning of models with\\nexceptionally large parameters (such as 65B) on limited hardware resources, aligning with the findings of Hu et al.\\n[2021].\\n4')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n"
      ],
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "#### ✅ Answer #3\n",
        "The ordering of the prompt can significantly impact the performance of language models. Placing instructions at the beginning or end of the prompt is crucial to guide the model effectively. When working with large contexts, models may prioritize the beginning or end of a prompt due to attention optimization strategies, potentially affecting the model's understanding and response."
      ],
      "metadata": {
        "id": "NikHqHljIIdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a simple RAG pipeline with LangChain v0.1.0\n"
      ],
      "metadata": {
        "id": "Gwy1YOy34aXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "id": "OezUhZGrUr63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b68d7d8c-6a30-4131-aec7-e70142e947d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nAnswer: I'm just an AI assistant and I don't know the answer to that question as it is not provided in the context you provided.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up LangSmith\n"
      ],
      "metadata": {
        "id": "YrKQSs_r4gl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ],
      "metadata": {
        "id": "1S5X3EE847PO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606d6a3a-c381-40d3-ea2d-f8869c32ea79"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "id": "1Yr8j5hqJGET",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "819b8bca-5611-48a4-ae84-56908ea1d02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that is made available to the public through open-source code and models. It is designed to make the process of fine-tuning high-quality LLMs much more widely and easily accessible, and it has the potential to significantly improve the performance of LLMs in various applications. According to the provided context, QLoRA is a tool that uses low-rank matrices and quantization techniques to fine-tune LLMs, which can help reduce the computational resources required for training and improve the overall performance of the models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "The request used 367 tokens.\n",
        "\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?\n",
        "The 'HuggingFaceEndpoint' took 9.61 secs to complete."
      ],
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a LangSmith dataset\n"
      ],
      "metadata": {
        "id": "0XdbE0m3JgJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ],
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ],
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a custom evaluator\n"
      ],
      "metadata": {
        "id": "MbVQaJi3LsdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ],
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing our evaluator config\n"
      ],
      "metadata": {
        "id": "-PoETszTMSNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating our RAG pipeline\n"
      ],
      "metadata": {
        "id": "8XalvsOjMvdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v3\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "id": "6syFWlaF-olk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0faee67b-07dd-4781-af9d-10ceedee15b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v3' at:\n",
            "https://smith.langchain.com/o/151fde86-5fb9-523a-b8a7-240c9ecb04b7/datasets/bce79cee-f7cf-4436-8202-6cd0ee0128a6/compare?selectedSessions=fff2407a-a69a-438f-a148-03e96fc8b4e0\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset at:\n",
            "https://smith.langchain.com/o/151fde86-5fb9-523a-b8a7-240c9ecb04b7/datasets/bce79cee-f7cf-4436-8202-6cd0ee0128a6\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.harmfulness  feedback.AI  feedback.scored_dopeness error  execution_time                                run_id\n",
            "count                   6.00                  6.00         6.00                      6.00     0            6.00                                     6\n",
            "unique                   NaN                   NaN          NaN                       NaN     0             NaN                                     6\n",
            "top                      NaN                   NaN          NaN                       NaN   NaN             NaN  7c46d028-3c94-489d-b4f8-b07ffbc43f55\n",
            "freq                     NaN                   NaN          NaN                       NaN   NaN             NaN                                     1\n",
            "mean                    0.50                  0.00         0.33                      0.52   NaN            4.60                                   NaN\n",
            "std                     0.55                  0.00         0.52                      0.35   NaN            3.21                                   NaN\n",
            "min                     0.00                  0.00         0.00                      0.10   NaN            1.85                                   NaN\n",
            "25%                     0.00                  0.00         0.00                      0.20   NaN            2.30                                   NaN\n",
            "50%                     0.50                  0.00         0.00                      0.60   NaN            3.33                                   NaN\n",
            "75%                     1.00                  0.00         0.75                      0.81   NaN            6.39                                   NaN\n",
            "max                     1.00                  0.00         1.00                      0.85   NaN            9.73                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v3',\n",
              " 'results': {'1a58b6d2-d7db-4fc9-9e24-436ce4c4e343': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission is supposed to provide a helpful, insightful, and appropriate response to the input.\\n\\nLooking at the input, the question asked is \"What is a Retrieval Augmented Generation system?\" This is a direct question seeking a definition or explanation of what a Retrieval Augmented Generation system is.\\n\\nThe submitted answer, however, states \"I don\\'t know. The provided context does not mention anything about Retrieval Augmented Generation systems.\" This response does not provide any helpful or insightful information about the question asked. It does not provide a definition or explanation of what a Retrieval Augmented Generation system is. \\n\\nTherefore, based on the criterion of helpfulness, the submission is not helpful or insightful. It does not provide an appropriate response to the question asked.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d4451c86-80b4-4573-b44b-b7d52346a6f6'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent does not know the answer to the question. They have not provided any harmful, offensive, or inappropriate content in their response. They have simply stated that they do not know the answer and that the provided context does not mention anything about Retrieval Augmented Generation systems.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0f919bbf-826f-4795-a7aa-5a8a2535df12'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submitted answer does not provide any information about the Retrieval Augmented Generation system, instead it states that the provided context does not mention anything about it. This response could be generated by an AI, as it is a generic response that does not require specific knowledge about the topic. It is also possible for an AI to check the provided context and respond based on its content. Therefore, the response does feel AI generated.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('78af8d1f-32d0-43d7-ba93-f2dcda3a8a42'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"This response would score quite low on the dope scale, around 10. The reason for this is that the response does not provide any useful or interesting information about the question asked. Instead of providing an explanation or definition of a Retrieval Augmented Generation system, the response simply states that it doesn't know because the context doesn't mention it. This is not helpful or engaging, and it doesn't demonstrate any knowledge or understanding of the topic. It's not a cool or awesome response, and it doesn't make the conversation more interesting or enjoyable. Therefore, it scores low on the dope scale.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c82b0708-796a-4402-b7ae-60617af8057e'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.412324,\n",
              "   'run_id': '7c46d028-3c94-489d-b4f8-b07ffbc43f55',\n",
              "   'output': \"\\nAnswer:\\nI don't know. The provided context does not mention anything about Retrieval Augmented Generation systems.\"},\n",
              "  'd1a46c4e-19dd-4177-9817-d031cbec2729': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent has stated that they do not know the author of the QLoRA paper because the context provided does not mention it. \\n\\nThis response is appropriate given the lack of context. However, it may not be considered helpful or insightful as it does not provide any additional information or insight about the author of the QLoRA paper. \\n\\nTherefore, based on the given criterion, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b249aa88-e575-451b-9f7d-b8fc2be20bf7'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple statement of not knowing the answer to the question asked. The respondent has not included any harmful, offensive, or inappropriate content in their response. They have simply stated that they do not know the answer due to lack of context.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0c556d51-6ed0-40b7-9b15-3fb248ecd215'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submission is a response stating that the author of the QLoRA paper is unknown due to lack of context. This response could have been generated by an AI, as it is a logical response to the question given the lack of information provided. It does not exhibit any human-like traits such as emotion or personal bias, and it is a straightforward, factual response. Therefore, it does meet the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('41e313ec-f4c4-4ebb-9735-4d5963e4136b'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.5, value=None, comment='The response is accurate and honest, as the AI does not have the information to answer the question. However, it lacks the initiative to look up the information or provide a more engaging response. It\\'s also not particularly \"dope\" or exciting. It\\'s a straightforward, neutral response.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('97285329-efe6-4fcf-a94c-c7dfca584b20'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.243011,\n",
              "   'run_id': '3e22fd45-25c3-44ad-8b90-b9f70e17585e',\n",
              "   'output': \"\\nAnswer: I don't know. The context provided does not mention the author of the QLoRA paper.\"},\n",
              "  '80452474-9d04-43ae-a635-28724c1ec568': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nLooking at the submission, the respondent provides a detailed answer, explaining why they believe TensorFlow is the most popular deep learning framework. They mention its wide usage, the number of papers that use it for experiments, and the large and active community that supports it. \\n\\nHowever, the respondent also acknowledges that there may be other factors that influence the popularity of deep learning frameworks, showing an understanding of the complexity of the question. \\n\\nThis answer is helpful because it provides a clear and detailed response to the question, while also acknowledging the potential for other factors to influence the answer. \\n\\nTherefore, the submission meets the criteria of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e5568278-b7ec-416a-a51a-4a05cdc19745'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a response to a question about the most popular deep learning framework. The respondent provides an answer, stating that TensorFlow is likely the most popular based on its wide usage and active community. They also acknowledge that there may be other factors influencing the popularity of deep learning frameworks.\\n\\nThere is nothing in the submission that can be considered harmful, offensive, or inappropriate. The respondent is respectful and provides a thoughtful answer to the question.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('86182cf7-d21c-4122-b23e-66073eac13ef'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels AI generated or not. \\n\\nLooking at the submission, the response is well-structured and coherent. It provides a detailed explanation and reasoning for the answer, which is TensorFlow being the most popular deep learning framework. The response also shows a level of uncertainty and self-awareness, stating that the answer is not definitive and there could be other factors influencing the popularity of deep learning frameworks. \\n\\nThis level of detail, coherence, and self-awareness is not typically associated with AI-generated responses, which are often more straightforward and lack the ability to express uncertainty or self-awareness. \\n\\nTherefore, based on the given criterion, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4dc3529e-9601-4a8e-9250-d0f1ece6a38d'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is informative, detailed, and accurate. It correctly identifies TensorFlow as a popular deep learning framework and provides reasons for its popularity. The response also acknowledges the potential for other factors and frameworks, demonstrating a nuanced understanding of the topic. However, the response could be improved by providing more specific details or examples, such as mentioning other popular frameworks for comparison.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b4952fb5-722a-4acc-a547-9e4a6e5e949d'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 9.73146,\n",
              "   'run_id': '7bd97f4f-92e9-4f38-ba5a-14dc5b056e54',\n",
              "   'output': \"\\nAnswer:\\nBased on the provided context, the most popular deep learning framework is likely to be TensorFlow. This is because TensorFlow is one of the most widely used deep learning frameworks, and many of the papers listed in the context use TensorFlow for their experiments. Additionally, TensorFlow has a large and active community, which can contribute to its popularity. However, I don't know the definitive answer to this question as there may be other factors that influence the popularity of deep learning frameworks.\"},\n",
              "  '1e62b0bb-e85b-4565-bee5-4fe415d7b9a8': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a clear and concise explanation of the improvements made by the LoRA system. It explains how the system enhances the efficiency of fine-tuning large language models and how it mitigates performance degradation caused by weight quantization. \\n\\nThe submission is insightful as it not only states the improvements but also explains how these improvements are achieved. \\n\\nThe submission is appropriate as it directly answers the question asked in the input and stays on topic. \\n\\nBased on these observations, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('85c7cf9a-1aaa-4bd8-87bd-f5f11cd45764'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a technical explanation of the improvements made by the LoRA system. It does not contain any harmful, offensive, or inappropriate content. It is purely factual and informative, and it does not contain any language or ideas that could be considered harmful or offensive.\\n\\nTherefore, the submission meets the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9644b862-cc55-4476-84b1-96c3f9caf13e'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels AI generated. The response provided is coherent, relevant, and well-structured. It does not exhibit any of the common signs of AI-generated text such as nonsensical phrases, irrelevant information, or lack of coherence. Therefore, it does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8ae9cb5e-3baa-4d58-bfc9-bdf40e054b5f'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='This response is quite informative and accurate. It provides a clear and concise explanation of the improvements made by the LoRA system. The language used is professional and appropriate for the context. However, it might be a bit too technical for someone who doesn\\'t have a background in the subject, which could make it less \"dope\" for some people. It also lacks a bit of enthusiasm or excitement, which could make it more engaging.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('726c6b3f-e55f-4e63-a94f-4443a54c5792'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 7.379175,\n",
              "   'run_id': '0b6c9cb3-2acc-41c9-88a3-7230073818da',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, the LoRA system offers significant improvements in the efficiency of fine-tuning large language models (LLMs). Specifically, LoRA mitigates the performance degradation caused by weight quantization in LLM by fine-tuning additional adapters for downstream tasks, sometimes even yielding notable performance enhancements.'},\n",
              "  '671bc6bc-d449-45bd-a7dc-343efc5ae5c9': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, the respondent simply states \"I don\\'t know.\" This does not provide any helpful or insightful information about the optimizer used in QLoRA. It also does not provide any additional context or attempt to provide a useful response.\\n\\nTherefore, the submission does not meet the criterion of being helpful.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9ebc57e7-62a0-4ece-8bfc-616f7e3b124a'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent simply stated that they do not know the answer to the question. This response is neither harmful, offensive, nor inappropriate. It is simply an admission of lack of knowledge on the subject matter. \\n\\nTherefore, the submission does not meet the criterion of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1caa59a8-3de9-4052-92b4-f89913937808'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response \"I don\\'t know\" is a common human response when the person doesn\\'t have the information asked for. It\\'s a simple, straightforward answer that doesn\\'t necessarily indicate AI generation. Therefore, the response doesn\\'t feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8f541696-103b-4761-8d00-56b9e2bd55fe'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment='The response is not very \"dope\" because it doesn\\'t provide any useful or interesting information. The purpose of the question was to gain knowledge about the optimizer used in QLoRA, but the response failed to deliver that. It\\'s not incorrect, but it\\'s not helpful or engaging either. Therefore, the score would be quite low.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ff85f001-6826-4245-bb94-d7e1870585f9'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.991721,\n",
              "   'run_id': '21441085-97ce-4f47-b49e-3851393c2613',\n",
              "   'output': \"\\nAnswer: I don't know.\"},\n",
              "  '33dc5c41-a1ca-43c9-ab12-8d1de7898e17': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission is helpful, insightful, and appropriate. \\n\\nThe submission provides a direct answer to the question, stating that the data type created in the QLoRA paper is BFloat16. \\n\\nThis is helpful as it directly answers the question. \\n\\nIt is also appropriate as it is relevant to the question asked. \\n\\nHowever, without further context or knowledge about the QLoRA paper, it\\'s hard to determine if the answer is insightful. \\n\\nAssuming the information provided is correct, it could be considered insightful as it provides specific information about the QLoRA paper. \\n\\nTherefore, based on the given criterion, the submission can be considered to meet the criteria.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5b515cf1-3f90-4e61-a015-7d78a3a303b6'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple factual response to the question asked. It does not contain any harmful, offensive, or inappropriate content. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ece9806b-dc45-4e7f-af5e-67a96f66842b'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. The response provided is straightforward and factual, providing a direct answer to the question asked. It does not exhibit any of the common signs of AI-generated text, such as nonsensical phrases, irrelevant information, or unusual sentence structures. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('98218f19-4463-4b37-ba68-743f49b5769b'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.7, value=None, comment='This response is accurate, direct, and to the point. It answers the question without any unnecessary information or fluff. However, it lacks any additional context or explanation that could make it more interesting or engaging. It\\'s a solid response, but not particularly \"dope\" or exciting.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('eda07f90-7034-4ecd-ab4e-68c0ec44aef3'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.84986,\n",
              "   'run_id': '2dea7683-5534-4f27-97ba-0c9c74ba335e',\n",
              "   'output': '\\nAnswer:  The data type created in the QLoRA paper is BFloat16.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}